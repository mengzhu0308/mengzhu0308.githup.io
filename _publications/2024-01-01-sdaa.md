---
title: "Spatial decomposition and aggregation for attention in convolutional neural networks"
collection: publications
category: manuscripts
permalink: /publication/2024-01-01-sdaa
excerpt: 'This paper is about channel-spatial mixed attention.'
date: 2024-01-01
venue: 'International Journal of Pattern Recognition and Artificial Intelligence'
slidesurl: ''
paperurl: 'https://mengzhu0308.github.io/mengzhu0308.githup.io/files/papers/2024-01-01-sdaa.pdf'
citation: 'Meng Zhu, Weidong Min*, Qi Wang, et al. PFLU and FPFLU: Two novel non-monotonic activation functions in convolutional neural networks. Neurocomputing, 2021, 429: 110-117. DOI: 10.1016/j.neucom.2020.11.068.'
---

# Abstract

Channel attention has been shown to improve the performance of deep convolutional neural networks eï¬ƒciently. Channel attention adaptively recalibrates the importance of each channel, determining what to attend to. However, channel attention only encodes inter-channel information but neglects the importance of positional information. Posi-tional information is crucial in determining where to attend to. To address this issue, we propose a novel channel-spatial attention method named Spatial-Decomposition-Aggregation Attention (SDAA) method. First, a high-axis spatial direction is decom-posed into multiple low-axis spatial directions. Then, a shared transformation sub-unit establishes attention in each low-axis space direction. Next, all the low-axis attention masks are aggregated into a high-axis attention mask. Finally, the generated high-axis attention mask is fused into the input features, thus enhancing the input features. Essen-tially, our method is a divide-and-conquer process. Experimental results demonstrate that our SDAA method outperforms the existing channel-spatial attention methods.