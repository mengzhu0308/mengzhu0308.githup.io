---
title: "AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate"
collection: publications
category: manuscripts
permalink: /publication/2025-11-18-adamx
excerpt: 'This paper is about optimization algorithms.'
date: 2025-11-18
venue: ''
slidesurl: ''
arXivurl: 'https://arxiv.org/abs/2511.13465'
GitHuburl: 'https://github.com/mengzhu0308/AdamX'
paperurl: 'https://mengzhu0308.github.io/mengzhu0308.githup.io/files/papers/2025-11-18-adamx.pdf'
citation: 'Meng Zhu, Quan Xiao, Weidong Min*. AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate. arXiv, 2025. URL https://arxiv.org/abs/2511.13465.'
---

# Abstract

Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.